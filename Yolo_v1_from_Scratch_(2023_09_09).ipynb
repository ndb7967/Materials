{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_Eovt3HuBxs"
      },
      "source": [
        "#### <b>필요한 라이브러리 불러오기</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0BiqmLeuBGU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from enum import Enum\n",
        "from functools import cmp_to_key\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.nn as nn\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o2GvEbPs3LJ"
      },
      "source": [
        "#### <b>VOC 객체 탐지 데이터 세트 라이브러리</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXSx6mK6vZM4"
      },
      "outputs": [],
      "source": [
        "categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "\n",
        "# 지금 코드는 최근에 작성된 코드\n",
        "# 옛날에는 VOC 방식으로 데이터를 많이 처리했는데, 최근에는 YOLO 형식으로 데이터를 처리\n",
        "#   * 전통적인 VOC 방식: (왼쪽 위 좌표, 오른쪽 아래 좌표)로 표현\n",
        "#   * 전통적인 YOLO 방식: (중간 좌표, 너비, 높이)로 표현\n",
        "# Dataset adapt for Yolo format (divided into cells)\n",
        "class VOCDataset(data.Dataset):\n",
        "    # PyTorch의 데이터 세트는 항상 __init__(), __len__(), __getitem()__ 구성\n",
        "    def __init__(self, dataset, train=True):\n",
        "        self.dataset = dataset\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.dataset[idx]\n",
        "\n",
        "        # 데이터를 리스트 형식으로 감싸기\n",
        "        if not isinstance(target['annotation']['object'], list):\n",
        "            target['annotation']['object'] = [target['annotation']['object']]\n",
        "\n",
        "        # 현재 이미지에서 물체(object)의 개수\n",
        "        count = len(target['annotation']['object'])\n",
        "\n",
        "        # 현재 불러온 이미지의 높이(height)와 너비(width) 가져오기\n",
        "        height, width = int(target['annotation']['size']['height']), int(target['annotation']['size']['width'])\n",
        "\n",
        "        # Image Augmentation\n",
        "        # 단순한 PyTorch Vision 라이브러리로 데이터 증진(augmentation)하지 않는다.\n",
        "        # <예시> 객체 탐지(object detection)에서는 회전이 되면 bounding box도 회전\n",
        "        if self.train:\n",
        "            # randomly scaling and translation up to 20% (최대 20%)\n",
        "            # 50% 확률로 랜덤하게 이미지를 ① 사이즈 변경하고, ② 조금 움직이도록 처리(패딩)\n",
        "            #   <정확히> 최대 20%까지 이미지를 축소, 최대 20%까지 이미지를 이동\n",
        "            if random.random() < 0.5:\n",
        "                # use random value to decide scaling factor on x and y axis\n",
        "                random_height = random.random() * 0.2 # 평균(기댓값)은 0.1\n",
        "                random_width = random.random() * 0.2 # 평균(기댓값)은 0.1\n",
        "                # use random value again to decide scaling factor for 4 borders\n",
        "                random_top = random.random() * random_height\n",
        "                random_left = random.random() * random_width\n",
        "                # calculate new width and height and position\n",
        "                # 위에서부터 거리(top), 왼쪽에서부터 거리(left), 이미지 높이(height), 이미지 너비(width)\n",
        "                top = random_top * height\n",
        "                left = random_left * width\n",
        "                height = height - random_height * height\n",
        "                width = width - random_width * width\n",
        "                # crop image\n",
        "                # 계산된 값에 따라서 데이터 증진이 완료된 이후의 이미지\n",
        "                img = torchvision.transforms.functional.crop(img, int(top), int(left), int(height), int(width))\n",
        "\n",
        "                # update target\n",
        "                for i in range(count):\n",
        "                    # 현재 사물의 bounding box (바운딩 박스)를 확인\n",
        "                    obj = target['annotation']['object'][i]\n",
        "                    # 이미지가 움직였으니까, bounding box도 거기에 맞게 움직인다. (위치 이동)\n",
        "                    # <심각한 의문점> bounding box에 대한 scaling은 왜 적용하지 않는가?\n",
        "                    obj['bndbox']['xmin'] = max(0, float(obj['bndbox']['xmin']) - left)\n",
        "                    obj['bndbox']['ymin'] = max(0, float(obj['bndbox']['ymin']) - top)\n",
        "                    obj['bndbox']['xmax'] = min(width, float(obj['bndbox']['xmax']) - left)\n",
        "                    obj['bndbox']['ymax'] = min(height, float(obj['bndbox']['ymax']) - top)\n",
        "\n",
        "            # adjust saturation randomly up to 150%\n",
        "            # 이미지 채도(saturation) 관련 데이터 증진\n",
        "            if random.random() < 0.5:\n",
        "                random_saturation = random.random() + 0.5\n",
        "                img = torchvision.transforms.functional.adjust_saturation(img, random_saturation)\n",
        "\n",
        "        # resize to 448 * 448 (원본 논문과 동일)\n",
        "        img = torchvision.transforms.functional.resize(img, (448, 448))\n",
        "\n",
        "        # update labels from absolute to relative\n",
        "        # YOLO로 변경하기 (YOLO는 bounding box를 이미지 내에서 [0, 1] 사이의 상대 위치로 표현)\n",
        "        #   * 반면에 VOC는 이미지 내에서의 픽셀 절댓값으로 표현\n",
        "        height, width = float(height), float(width)\n",
        "\n",
        "        # 원본 VOC 예시) {'xmin': '141', 'ymin': '50', 'xmax': '500', 'ymax': '330'} 같은 형식\n",
        "        # YOLO 예시) {'x': 0.6, 'y': 0.4, 'width': 0.4, 'height': 0.3} 같은 형식\n",
        "        for i in range(count):\n",
        "            # 값을 [0, 1]사이의 값으로(상대 값으로) 변경\n",
        "            obj = target['annotation']['object'][i]\n",
        "            obj['bndbox']['xmin'] = float(obj['bndbox']['xmin']) / width\n",
        "            obj['bndbox']['ymin'] = float(obj['bndbox']['ymin']) / height\n",
        "            obj['bndbox']['xmax'] = float(obj['bndbox']['xmax']) / width\n",
        "            obj['bndbox']['ymax'] = float(obj['bndbox']['ymax']) / height\n",
        "\n",
        "        # 정답 레이블(label encoding)\n",
        "        # [bounding box 1, bounding box 2, 클래스 정보(20개)]\n",
        "        # <핵심> bounding box 1과 bounding box 2는 정확히 동일하게 구성한다.\n",
        "        # (원래) [{'name': '', 'xmin': '', 'ymin': '', 'xmax': '', 'ymax': '', }, {}, {}, ...]\n",
        "        # (바꾼 뒤) [x, y  (relative to cell), width, height, 1 if exist (confidence),\n",
        "        #   x, y  (relative to cell), width, height, 1 if exist (confidence),\n",
        "        #   one-hot encoding of 20 categories]\n",
        "        label = torch.zeros((7, 7, 30))\n",
        "        for i in range(count):\n",
        "            obj = target['annotation']['object'][i]\n",
        "            xmin = obj['bndbox']['xmin']\n",
        "            ymin = obj['bndbox']['ymin']\n",
        "            xmax = obj['bndbox']['xmax']\n",
        "            ymax = obj['bndbox']['ymax']\n",
        "            name = obj['name']\n",
        "\n",
        "            # 바운딩 박스의 크기가 0인 경우 무시\n",
        "            if xmin == xmax or ymin == ymax:\n",
        "                continue\n",
        "            # 바운딩 박스가 이미지 밖으로 벗어난 경우 무시\n",
        "            if xmin >= 1 or ymin >= 1 or xmax <= 0 or ymax <= 0:\n",
        "                continue\n",
        "\n",
        "            # 중간 위치(center x, center y)\n",
        "            x = (xmin + xmax) / 2.0\n",
        "            y = (ymin + ymax) / 2.0\n",
        "\n",
        "            # 너비와 높이 계산\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "\n",
        "            # 그리드가 7 X 7이므로, 현재의 (x, y)가 49개 중에서 몇 번째 격자인지\n",
        "            # 격자의 인덱스 값이 (x_idx, y_idx)를 의미\n",
        "            xidx = math.floor(x * 7.0) # 너비(왼쪽에서부터)\n",
        "            yidx = math.floor(y * 7.0) # 높이(위에서부터)\n",
        "\n",
        "            # According to the paper\n",
        "            # if multiple objects exist in the same cell\n",
        "            # pick the one with the largest area\n",
        "            # 현재 시점에서 label은 (7, 7, 30) 리스트\n",
        "            # 하나의 bounding box는 [x, y, width, height, 물체 존재 여부(T/F)] 형태를 가짐\n",
        "            if label[yidx][xidx][4] == 1: # already have object\n",
        "                # 현재 정답 레이블을 차지하고 있던 물체의 크기보다 더 큰 것이 발견 된 경우\n",
        "                if label[yidx][xidx][2] * label[yidx][xidx][3] < width * height:\n",
        "                    use_data = True # 그것으로 대체\n",
        "                else: use_data = False\n",
        "            else: use_data = True # 현재 위치에 아무 물체도 없었던 경우\n",
        "\n",
        "            if use_data:\n",
        "                # [0, 5]인 이유는 두 개의 bounding box가 같은 물체를 가리키도록 해야 하므로\n",
        "                for offset in [0, 5]:\n",
        "                    # \"전체 이미지 기준의 위치\"를 \"현재 셀 기준의 위치\"로 좌표 값 변경\n",
        "                    # Transforming image relative coordinates to cell relative coordinates:\n",
        "                    # x - idx / 7.0 = x_cell / cell_count (7.0)\n",
        "                    # => x_cell = x * cell_count - idx = x * 7.0 - idx\n",
        "                    # y is the same\n",
        "                    # 현재 코드는 현재의 셀 안에서 [0, 1]로 물체의 중심의 위치를 표현\n",
        "                    # <이 코드의 가정> 현재의 셀 안에 무조건 물체의 중심이 있을 것이라는 것\n",
        "                    #   (질문) 특정한 셀은 자신의 셀을 벗어난 위치의 물체를 찾을 수 없다.\n",
        "                    #   (재반박) 그래도 괜찮다. 벗어난 위치에 있는 다른 셀이 그것 찾을 것이다.\n",
        "                    label[yidx][xidx][0 + offset] = x * 7.0 - xidx\n",
        "                    label[yidx][xidx][1 + offset] = y * 7.0 - yidx\n",
        "                    label[yidx][xidx][2 + offset] = width\n",
        "                    label[yidx][xidx][3 + offset] = height\n",
        "                    label[yidx][xidx][4 + offset] = 1\n",
        "                # 결과적으로 모델의 예측 값은 모두 \"셀 내부에서의\" 상대 위치\n",
        "                label[yidx][xidx][10 + categories.index(name)] = 1\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxx25HcOs-bU"
      },
      "outputs": [],
      "source": [
        "# Raw Dataset for testing mAP, Precision and Recall\n",
        "# VOC 방식으로 작성된 데이터 세트를 YOLO 형식으로 단순히 변경\n",
        "class VOCRawTestDataset(data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.dataset[idx]\n",
        "\n",
        "        if not isinstance(target['annotation']['object'], list):\n",
        "            target['annotation']['object'] = [target['annotation']['object']]\n",
        "\n",
        "        # 현재 이미지에서 물체(object)의 개수\n",
        "        count = len(target['annotation']['object'])\n",
        "\n",
        "        # 현재 불러온 이미지의 높이(height)와 너비(width) 가져오기\n",
        "        height, width = int(target['annotation']['size']['height']), int(target['annotation']['size']['width'])\n",
        "\n",
        "        # resize to 448 * 448 (원본 논문과 동일)\n",
        "        img = torchvision.transforms.functional.resize(img, (448, 448))\n",
        "\n",
        "        # update labels from absolute to relative\n",
        "        height, width = float(height), float(width)\n",
        "\n",
        "        ret_targets = []\n",
        "\n",
        "        # 값을 YOLO 형식으로 [0, 1]사이의 값으로(상대 값으로) 변경\n",
        "        for i in range(count):\n",
        "            obj = target['annotation']['object'][i]\n",
        "\n",
        "            ret_targets.append({\n",
        "                'xmin': float(obj['bndbox']['xmin']) / width,\n",
        "                'ymin': float(obj['bndbox']['ymin']) / height,\n",
        "                'xmax': float(obj['bndbox']['xmax']) / width,\n",
        "                'ymax': float(obj['bndbox']['ymax']) / height,\n",
        "                'category': categories.index(obj['name']),\n",
        "                'difficult': obj['difficult'] == '1',\n",
        "            })\n",
        "\n",
        "        return img, json.dumps(ret_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuwrgMzbtxH4"
      },
      "outputs": [],
      "source": [
        "def load_data_voc(batch_size, num_workers=0, persistent_workers=False, download=False, test_shuffle=True):\n",
        "    \"\"\"\n",
        "     Loads the Pascal VOC dataset.\n",
        "     :return: train_iter, test_iter, test_raw_iter\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    trans = [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ]\n",
        "    trans = torchvision.transforms.Compose(trans)\n",
        "    # Pascal VOC 데이터 세트는 2007년도, 2012년도 데이터 세트가 많이 사용\n",
        "    #   * 학습할 때: 2007년도와 2012년도에서 \"학습(training) + 검증(valiidation)\" 데이터로 학습\n",
        "    #   * 테스트할 때: 2007년도와 2012년도의 테스트(test) 데이터로 성능 평가\n",
        "    voc2007_trainval = torchvision.datasets.VOCDetection(root='../data/VOCDetection/', year='2007', image_set='trainval', download=download, transform=trans)\n",
        "    voc2007_test = torchvision.datasets.VOCDetection(root='../data/VOCDetection/', year='2007', image_set='test', download=download, transform=trans)\n",
        "    voc2012_train = torchvision.datasets.VOCDetection(root='../data/VOCDetection/', year='2012', image_set='train', download=download, transform=trans)\n",
        "    voc2012_val = torchvision.datasets.VOCDetection(root='../data/VOCDetection/', year='2012', image_set='val', download=download, transform=trans)\n",
        "    # <하지만> 현재 코드에서는 학습(voc2007의 train + val + test, voc2012의 train), 평가(voc2012_val)\n",
        "    return (\n",
        "        data.DataLoader(VOCDataset(data.ConcatDataset([voc2007_trainval, voc2007_test, voc2012_train]), train=True),\n",
        "            batch_size, shuffle=True, num_workers=num_workers, persistent_workers=persistent_workers),\n",
        "        data.DataLoader(VOCDataset(voc2012_val, train=False),\n",
        "            batch_size, shuffle=test_shuffle, num_workers=num_workers, persistent_workers=persistent_workers),\n",
        "        data.DataLoader(VOCRawTestDataset(voc2012_val),\n",
        "            batch_size, shuffle=test_shuffle, num_workers=num_workers, persistent_workers=persistent_workers)\n",
        "\t  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh3sScl29UXf"
      },
      "source": [
        "#### <b>모델 가중치 초기화 관련 라이브러리</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMToR4Q_9bjn"
      },
      "outputs": [],
      "source": [
        "# From https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n",
        "def weight_init(m):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "        model = Model()\n",
        "        model.apply(weight_init)\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm3d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.LSTMCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-IkstHc9bMH"
      },
      "source": [
        "#### <b>객체 탐지 모델 평가 관련 라이브러리</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXmfb0259XE_"
      },
      "outputs": [],
      "source": [
        "# 모델의 성능을 평가할 때는 threshold 값을 0부터 1까지 늘려가며 측정\n",
        "class InterpolationMethod(Enum):\n",
        "    Interpolation_11 = 1 # [0, 1]에서 0.1씩 늘리는 경우 → 총 11개의 포인트\n",
        "    Interpolation_101 = 2 # [0, 1]에서 0.01씩 늘리는 경우 → 총 101개의 포인트\n",
        "\n",
        "\n",
        "class CalculationMetrics():\n",
        "    def __init__(self, IoU: float, confidence: float, mustbe_FP: bool, is_difficult: bool):\n",
        "        \"\"\"Initialization for `CalculationMetrics`\n",
        "\n",
        "        Args:\n",
        "            IoU (float): intersection over union with ground truth\n",
        "            confidence (float): detection confidence\n",
        "            mustbe_FP (bool): if there is already another detection having higher IoU with the same ground truth, then this detection must be False Positive\n",
        "            is_difficult (bool): if the ground truth is difficult, then this detection may be neglected in certain cases\n",
        "        \"\"\"\n",
        "        self.IoU = IoU\n",
        "        self.confidence = confidence\n",
        "        self.mustbe_FP = mustbe_FP\n",
        "        self.is_difficult = is_difficult\n",
        "\n",
        "\n",
        "# 정렬 기준 함수: ① confidence가 높은 순으로 정렬, ② IoU가 높은 순으로 정렬\n",
        "def compare_metrics(metrics1: CalculationMetrics, metrics2: CalculationMetrics):\n",
        "    if metrics1.confidence == metrics2.confidence:\n",
        "        return metrics2.IoU - metrics1.IoU\n",
        "    return metrics2.confidence - metrics1.confidence\n",
        "\n",
        "\n",
        "# mAP (mean Average Precision)\n",
        "# Object Detection 분야에서는 AP (Average Precision)이 0.5만 되어도 꽤 훌륭하다.\n",
        "#   1) 물체의 위치를 정확히 예측하고(IoU가 충분히 높아야 하며)\n",
        "#   2) 그와 동시에 클래스까지 정확히 예측해야 TP (True Positive)가 되기 때문이다.\n",
        "# 여기에서 mean의 의미는 각 클래스(category)에 따른 평균을 의미한다.\n",
        "class ObjectDetectionMetricsCalculator():\n",
        "    # data\n",
        "    # [       # classes\n",
        "    #   {\n",
        "    #      \"data\": [     # data\n",
        "    #         <CalculationMetrics>\n",
        "    #      ],\n",
        "    #      \"detection\": <int>,\n",
        "    #      \"truth\": <int>\n",
        "    #   }\n",
        "    # ]\n",
        "    def __init__(self, num_classes: int, confidence_thres: float):\n",
        "        \"\"\"ObjectDetectionMetricsCalculator Initialization\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): number of classes detector can classify\n",
        "            confidence_thres (float): confidence threshold. if the detection's confidence is smaller than the threshold, it would not be counted as a detection. In other words, it would be neither TP nor FP.\n",
        "        \"\"\"\n",
        "        # initialize data\n",
        "        self.data = [{\"data\": [], \"detection\": 0, \"truth\": 0} for _ in range(num_classes)]\n",
        "        self.confidence_thres = confidence_thres\n",
        "\n",
        "    def add_image_data(self, pred: torch.Tensor, truth: str):\n",
        "        \"\"\"Add new image data for calculating metrics\n",
        "\n",
        "        Args:\n",
        "            pred (torch.Tensor): detection prediction\n",
        "            truth (str): ground truth json string\n",
        "        \"\"\"\n",
        "        pred = pred.reshape(-1, 30) # 모델의 예측 결과 (물체의 개수, 30)\n",
        "        truth = json.loads(truth) # 정답 물체\n",
        "\n",
        "        choose_truth_index = [None for _ in range(pred.shape[0])]\n",
        "        iou = [0 for _ in range(pred.shape[0])]\n",
        "\n",
        "        # 모델이 예측한 바운딩 박스(bounding box)를 하나씩 확인하며\n",
        "        for i in range(pred.shape[0]):\n",
        "            # 가장 높은 클래스(class)의 confidence score(점수)와 카테고리(category) 계산\n",
        "\n",
        "            # 해당 사물이 특정한 클래스에 속할 확률 Pr(Class|Object)\n",
        "            score, cat = pred[i][10:30].max(dim=0)\n",
        "            # 사물이 있을 확률 Pr(Object)\n",
        "            confidence = pred[i][4]\n",
        "\n",
        "            # <현재의 바운딩 박스에 대하여>\n",
        "            # filter by confidence threshold\n",
        "            # Pr(Object) * Pr(Class|Object) = Pr(Object, Class)가 threshold보다 작으면 기각\n",
        "            # Yolo v1 논문에서는 Pr(Object, Class) = Pr(Class)라고 가정\n",
        "            if confidence * score < self.confidence_thres: continue\n",
        "\n",
        "            # <핵심> 여기까지 왔으면 모델이 \"사물이 있다고\" 주장하는 것이다.\n",
        "            # 즉, (FP + TP)이 모두 포함된다.\n",
        "\n",
        "            # 모델이 예측한 현재의 bounding box의 위치(x, y)와 너비(w) 높이(h)\n",
        "            x, y, w, h = pred[i][0:4] # (cell relative coordinates)\n",
        "            # 이때 (x, y, w, h)는 현재의 셀(cell) 안에서의 상대 위치를 의미한다.\n",
        "            # 만약 셀의 정중앙에 있다면 x = 0.5, y = 0.5인 것이다.\n",
        "\n",
        "            # calculate cell index\n",
        "            # 7 X 7 격자에서의 셀(cell)의 인덱스 계산\n",
        "            xidx = i % 7\n",
        "            yidx = i // 7\n",
        "\n",
        "            # <핵심> transform cell relative coordinates to image relative coordinates\n",
        "            # 셀 내부에서의 상대 위치를 전체 이미지에 대한 상대 위치로 변환\n",
        "            # hat의 의미는 \"예측된(predicted)\"의 의미를 가진다.\n",
        "\n",
        "            # 전체 이미지의 왼쪽 위에서부터 오른쪽으로 얼마나 떨어져 있는지 [0, 1] 범위로 표현\n",
        "            xhat = (x + xidx) / 7.0\n",
        "            # 전체 이미지의 왼쪽 위에서부터 아래쪽으로 얼마나 떨어져 있는지 [0, 1] 범위로 표현\n",
        "            yhat = (y + yidx) / 7.0\n",
        "\n",
        "            # [전체 이미지에서의] 예측된 bounding box를 의미\n",
        "            # 왼쪽 위(xmin_hat, ymin_hat), 오른쪽 아래(xmax_hat, ymax_hat)\n",
        "            xmin_hat = xhat - w / 2\n",
        "            xmax_hat = xhat + w / 2\n",
        "            ymin_hat = yhat - h / 2\n",
        "            ymax_hat = yhat + h / 2\n",
        "\n",
        "            # 해당 이미지에 있는 실제 모든 \"물체\"를 하나씩 확인하며\n",
        "            for j in range(len(truth)):\n",
        "                bbox = truth[j]\n",
        "                # judge whether is same class\n",
        "                # 현재 물체와 동일한 클래스(class)인 경우에만 아래를 계산\n",
        "                if cat != bbox['category']: continue\n",
        "                # 현재 물체가 존재하는 bounding box\n",
        "                xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
        "                # calculate IoU\n",
        "                # 실질적으로 IoU 계산하기\n",
        "                wi = min(xmax, xmax_hat) - max(xmin, xmin_hat)\n",
        "                wi = max(wi, 0)\n",
        "                hi = min(ymax, ymax_hat) - max(ymin, ymin_hat)\n",
        "                hi = max(hi, 0)\n",
        "                intersection = wi * hi\n",
        "                union = (xmax - xmin) * (ymax - ymin) + (xmax_hat - xmin_hat) * (ymax_hat - ymin_hat) - intersection\n",
        "                this_iou = intersection / (union + 1e-6) # 0으로 나누어지는 문제 예방\n",
        "                # determine whether to choose this ground truth\n",
        "                # 이 ground-truth를 선택할 것인지 결정\n",
        "                # <핵심> 현재 내가 예측한 bounding box와 가장 많이 겹치는 사물 찾는 역할 수행\n",
        "                #   - ① 해당 사물과의 IoU 계산 (가장 IoU가 높은 정답 사물에 대하여)\n",
        "                #   - ② 가장 많이 겹치는 사물의 index(j) 계산\n",
        "                if iou[i] is None: choose = choose_truth_index\n",
        "                elif iou[i] < this_iou: choose = True # 최댓값 찾기\n",
        "                else: choose = False\n",
        "                # if choose, assign value\n",
        "                if choose:\n",
        "                    iou[i] = this_iou\n",
        "                    choose_truth_index[i] = j\n",
        "\n",
        "        # init a bool array for judging mustbe_FP later\n",
        "        truth_chosen = [False for _ in range(len(truth))]\n",
        "\n",
        "        # sort according to IoU\n",
        "        # IoU에 따라서 내림차순 정렬(IoU가 큰 것부터 계산)\n",
        "        # 모델이 예측한 bounding box 중에서 IoU가 큰 것부터 나열\n",
        "        sort_idx = np.argsort(iou)[::-1]\n",
        "\n",
        "        # add into metrics\n",
        "        # 모델이 bounding box 중에서 IoU가 큰 것부터 하나씩 확인하며\n",
        "        #   * 이 중에서 특정 bounding box는 (당연히) IoU가 0일 수도 있다.\n",
        "        for i in sort_idx:\n",
        "            # 해당 사물이 특정한 클래스에 속할 확률 Pr(Class|Object)\n",
        "            score, cat = pred[i][10:30].max(dim=0)\n",
        "            # 사물이 있을 확률 Pr(Object)\n",
        "            confidence = pred[i][4]\n",
        "\n",
        "            # <현재의 바운딩 박스에 대하여>\n",
        "            # filter by confidence threshold\n",
        "            # Pr(Object) * Pr(Class|Object) = Pr(Object, Class)가 threshold보다 작으면 기각\n",
        "            # Yolo v1 논문에서는 Pr(Object, Class) = Pr(Class)라고 가정\n",
        "            if confidence * score < self.confidence_thres: continue\n",
        "\n",
        "            # <핵심> 여기까지 왔으면 모델이 \"사물이 있다고\" 주장하는 것이다.\n",
        "            # 즉, (FP + TP)이 모두 포함된다.\n",
        "\n",
        "            truth_index = choose_truth_index[i]\n",
        "            # 동일한 클래스 중에서 IoU > 0인 물체(실제 정답)가 하나도 없는 경우\n",
        "            if truth_index == None:\n",
        "                mustbe_FP = True # 없는 것을 있다고 했으므로 FP (False Positive)\n",
        "                is_difficult = False\n",
        "            # \"현재 물체\"를 다른 bounding box가 선택한 적이 있다면\n",
        "            elif truth_chosen[truth_index]:\n",
        "                # 현재 bounding box의 IoU가 더 낮으니까, 현재 bounding box는 FP\n",
        "                mustbe_FP = True\n",
        "                is_difficult = truth[choose_truth_index[i]]['difficult']\n",
        "            else:\n",
        "                # 현재 bounding box가 현재의 물체를 최종적으로 선택\n",
        "                mustbe_FP = False\n",
        "                truth_chosen[choose_truth_index[i]] = True\n",
        "                is_difficult = truth[choose_truth_index[i]]['difficult']\n",
        "\n",
        "            # Average Precision (AP)를 계산할 때는 카테고리(클래스)별로 계산하므로\n",
        "            # 현재 이미지의 특정한 클래스에 현재의 모델의 예측 bounding box의 성능(IoU, confidence 값, FP 여부)\n",
        "            self.data[cat]['data'].append(CalculationMetrics(iou[i], float(confidence * score), mustbe_FP, is_difficult))\n",
        "\n",
        "            # update detection statistics\n",
        "            self.data[cat]['detection'] += 1\n",
        "\n",
        "        # update ground truth statistics\n",
        "        # 현재 이미지에 존재하는 실제 사물(ground-truth)들을 하나씩 확인하며\n",
        "        for bbox in truth:\n",
        "            # VOC 데이터 세트 중에서 difficulty로 레이블된 것은 제외 (실제 대회에서도 사용하지 X)\n",
        "            if bbox['difficult']: continue\n",
        "            self.data[bbox['category']]['truth'] += 1 # 개수 세기\n",
        "\n",
        "\n",
        "    def calculate_precision_recall(self, iou_thres: float, class_idx: int) -> list:\n",
        "        \"\"\"Calculate Precision-Recall Data according to IoU threshold\n",
        "\n",
        "        Args:\n",
        "            iou_thres (float): IoU threshold → 이거 이상으로 ground-truth와 bounding box가 겹쳐야 TP 인정\n",
        "            class_idx (int): Class Index\n",
        "\n",
        "        Returns:\n",
        "            list: `[{\"precision\": <precision>, \"recall\": <recall>}]`\n",
        "        \"\"\"\n",
        "        ret = []\n",
        "        # retrieve count (실제 사물이 존재하는 개수)\n",
        "        truth_cnt = self.data[class_idx]['truth'] # (FN + TP)\n",
        "        # accumulated TP\n",
        "        acc_TP = 0\n",
        "        # accumulated difficult count\n",
        "        acc_difficult = 0\n",
        "        # sort metrics by confidence\n",
        "        # <핵심> 왜 정렬했는가? 없어도 결과 똑같을 것 같다.\n",
        "        data = sorted(self.data[class_idx]['data'], key=cmp_to_key(compare_metrics))\n",
        "        for i, metrics in enumerate(data):\n",
        "            if metrics.IoU >= iou_thres and not metrics.mustbe_FP and not metrics.is_difficult:\n",
        "                acc_TP += 1\n",
        "            if metrics.is_difficult:\n",
        "                acc_difficult += 1\n",
        "            # 코드상으로는 \"difficulty\"가 아닌 게 한 번이라도 나오면 세겠다는 의미(True)\n",
        "            if i + 1 - acc_difficult > 0:\n",
        "                ret.append({\n",
        "                    # TP / (FP + TP) = 맞힌 개수 / 모델이 예측한 개수\n",
        "                    'precision': acc_TP / (i + 1 - acc_difficult),\n",
        "                    # TP / (FN + TP) = 맞힌 개수 / 실제 물체의 개수\n",
        "                    'recall': acc_TP / truth_cnt\n",
        "                })\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def calculate_average_precision(self, iou_thres: float, class_idx: int, itpl_option: InterpolationMethod) -> float:\n",
        "        \"\"\"Calculate Average Precision (AP)\n",
        "\n",
        "        Args:\n",
        "            iou_thres (float): IoU Threshold\n",
        "            class_idx (int): Class Index\n",
        "            itpl_option (InterpolationMethod): Interpolation Method\n",
        "\n",
        "        Returns:\n",
        "            float: AP of specified class using provided interpolation method\n",
        "        \"\"\"\n",
        "        prl = self.calculate_precision_recall(iou_thres=iou_thres, class_idx=class_idx)\n",
        "\n",
        "        if itpl_option == InterpolationMethod.Interpolation_11:\n",
        "            intp_pts = [0.1 * i for i in range(11)]\n",
        "        elif itpl_option == InterpolationMethod.Interpolation_101:\n",
        "            intp_pts = [0.01 * i for i in range(101)]\n",
        "        else:\n",
        "            raise Exception('Unknown Interpolation Method')\n",
        "\n",
        "        max_dict = {}\n",
        "        gmax = 0\n",
        "\n",
        "        for pr in prl[::-1]:\n",
        "            gmax = max(gmax, pr['precision'])\n",
        "            max_dict[pr['recall']] = gmax\n",
        "\n",
        "        if len(max_dict) < 1: return 0.\n",
        "\n",
        "        max_keys = max_dict.keys()\n",
        "        max_keys = sorted(max_keys)\n",
        "\n",
        "        key_ptr = len(max_keys) - 2\n",
        "        last_key = max_keys[-1]\n",
        "\n",
        "        AP = 0\n",
        "\n",
        "        for query in intp_pts[::-1]:\n",
        "            if key_ptr < 0:\n",
        "                if query > last_key:\n",
        "                    ans = 0\n",
        "                else:\n",
        "                    ans = max_dict[last_key]\n",
        "            else:\n",
        "                if query > last_key:\n",
        "                    ans = 0\n",
        "                elif query > max_keys[key_ptr]:\n",
        "                    ans = max_dict[last_key]\n",
        "                else:\n",
        "                    while key_ptr >= 0:\n",
        "                        if query > max_keys[key_ptr]:\n",
        "                            break\n",
        "                        last_key = max_keys[key_ptr]\n",
        "                        key_ptr -= 1\n",
        "                    ans = max_dict[last_key]\n",
        "            AP += ans\n",
        "\n",
        "        AP /= len(intp_pts)\n",
        "        return AP\n",
        "\n",
        "\n",
        "    def calculate_mAP(self, iou_thres: float, itpl_option: InterpolationMethod) -> float:\n",
        "        \"\"\"calculate mAP using given IoU threshold and interpolation method\n",
        "\n",
        "            Args:\n",
        "                iou_thres (float): IoU threshold\n",
        "                itpl_option (InterpolationMethod): Interpolation Method\n",
        "\n",
        "            Returns:\n",
        "                float: Mean Average Precision (mAP)\n",
        "        \"\"\"\n",
        "        mAP = 0\n",
        "        for c in range(len(self.data)):\n",
        "            mAP += self.calculate_average_precision(iou_thres, c, itpl_option)\n",
        "        mAP /= len(self.data)\n",
        "\n",
        "        return mAP\n",
        "\n",
        "    def calculate_VOCmAP(self) -> float:\n",
        "        \"\"\"calculate VOCmAP: mAP with IoU thres = .5, interpolate by 0.1\n",
        "\n",
        "        Returns:\n",
        "            float: VOC mAP\n",
        "        \"\"\"\n",
        "        return self.calculate_mAP(0.5, InterpolationMethod.Interpolation_11)\n",
        "\n",
        "\n",
        "    def calculate_COCOmAP50(self) -> float:\n",
        "        \"\"\"calculate COCO mAP @50 (AP@.5): expand VOCmAP50, interpolate by 0.01\n",
        "\n",
        "        Returns:\n",
        "            float: AP@.5\n",
        "        \"\"\"\n",
        "        return self.calculate_mAP(0.5, InterpolationMethod.Interpolation_101)\n",
        "\n",
        "\n",
        "    def calculate_COCOmAP75(self) -> float:\n",
        "        \"\"\"calculate COCO mAP @75 (AP@.75): AP@.5, but with IoU thres = .75\n",
        "\n",
        "        Returns:\n",
        "            float: AP@.75\n",
        "        \"\"\"\n",
        "        return self.calculate_mAP(0.75, InterpolationMethod.Interpolation_101)\n",
        "\n",
        "\n",
        "    def calculate_COCOmAP(self) -> float:\n",
        "        \"\"\"calculate COCO mAP: expand AP@.5 and AP@.75. IoU thres from .5 to .95\n",
        "\n",
        "        Returns:\n",
        "            float: COCO mAP\n",
        "        \"\"\"\n",
        "        ious = [0.5 + 0.05 * i for i in range(10)]\n",
        "        coco_map = 0\n",
        "        for iou in ious:\n",
        "            coco_map += self.calculate_mAP(iou, InterpolationMethod.Interpolation_101)\n",
        "        coco_map /= len(ious)\n",
        "        return coco_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxC1dX_9Eru"
      },
      "source": [
        "#### <b>Visualization 라이브러리</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "takfqwGq9IJV"
      },
      "outputs": [],
      "source": [
        "categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "# 20 random color for labeling\n",
        "colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for _ in range(20)]\n",
        "\n",
        "\n",
        "def draw_precision_recall(pr_data: list, class_idx: Optional[int]=None):\n",
        "    \"\"\"Draw Precision-Recall Curve\n",
        "\n",
        "    Args:\n",
        "        pr_data (list): Precision Recall Curve Data\n",
        "        class_idx (Optional[int]): Class index, used to render title\n",
        "    \"\"\"\n",
        "    p = [data['precision'] for data in pr_data]\n",
        "    r = [data['recall'] for data in pr_data]\n",
        "\n",
        "    plt.plot(r, p, 'o-', color='r')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "\n",
        "    if class_idx is not None:\n",
        "        plt.title(categories[class_idx])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def draw_box(img, x, y, w, h, score, category):\n",
        "    \"\"\"\n",
        "    Tool function to draw confidence box on image\n",
        "    :param img: numpy image to be rendered\n",
        "    :param x, y: relative center of box\n",
        "    :param w, h: relative size of box\n",
        "    :param score: confidence score\n",
        "    :param category: category of object\n",
        "    :return: image with box\n",
        "    \"\"\"\n",
        "    height = img.shape[0] * h\n",
        "    width = img.shape[1] * w\n",
        "    left = img.shape[1] * x - width / 2\n",
        "    top = img.shape[0] * y - height / 2\n",
        "\n",
        "    color = colors[category]\n",
        "    text = categories[category] + \" \" + str(float(score))\n",
        "    cv2.rectangle(img, (int(left), int(top)), (int(left + width), int(top + height)), color, 2)\n",
        "\n",
        "    text_size, baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "    p1 = (int(left), int(top) - text_size[1])\n",
        "\n",
        "    cv2.rectangle(img,\n",
        "        (p1[0] - 2//2, p1[1] - 2 - baseline),\n",
        "        (p1[0] + text_size[0], p1[1] + text_size[1]), color, -1)\n",
        "    cv2.putText(img, text,\n",
        "        (p1[0], p1[1] + baseline),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def draw_detection_result(img, pred, raw=False, thres=0.1):\n",
        "    \"\"\"\n",
        "    Tool function to draw detection result on image\n",
        "    :param img: numpy image to be rendered\n",
        "    :param pred: detection result (torch.Tensor)\n",
        "    :param raw: if true, two dimension of detection results will be rendered (5 * 2 + 20)\n",
        "    :param thres: threshold to filter out low confidence boxes\n",
        "    :return: image with detection result\n",
        "    \"\"\"\n",
        "    if raw:\n",
        "        offsets = [0, 5]\n",
        "    else: offsets = [0]\n",
        "\n",
        "    for offset in offsets:\n",
        "        pred = pred.reshape((-1, 30))\n",
        "        for i in range(pred.shape[0]):\n",
        "            x, y, w, h, iou = pred[i][0 + offset : 5 + offset]\n",
        "\n",
        "            # calculate cell index\n",
        "            xidx = i % 7\n",
        "            yidx = i // 7\n",
        "\n",
        "            # transform cell relative coordinates to image relative coordinates\n",
        "            x = (x + xidx) / 7.0\n",
        "            y = (y + yidx) / 7.0\n",
        "\n",
        "            score, cat = pred[i][10:30].max(dim=0)\n",
        "            if iou * score < thres: continue\n",
        "            img = draw_box(img, x, y, w, h, score * iou, cat)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def draw_ground_truth(img, truth):\n",
        "    \"\"\"\n",
        "    Tool function to draw ground truth\n",
        "    :param img: numpy image to be rendered\n",
        "    :param pred: truth bbox in json format (str)\n",
        "    :return: image with ground truth bbox\n",
        "    \"\"\"\n",
        "    pred = json.loads(truth)\n",
        "    for bbox in pred:\n",
        "        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
        "        w = xmax - xmin\n",
        "        h = ymax - ymin\n",
        "        x = (xmin + xmax) / 2\n",
        "        y = (ymin + ymax) / 2\n",
        "        img = draw_box(img, x, y, w, h, 1, bbox['category'])\n",
        "    return img\n",
        "\n",
        "\n",
        "def tensor_to_PIL(img):\n",
        "    \"\"\"Convert a tensor into a PIL image\"\"\"\n",
        "    to_pil = torchvision.transforms.ToPILImage()\n",
        "    return to_pil(img.cpu()).convert('RGB')\n",
        "\n",
        "\n",
        "def tensor_to_cv2(img):\n",
        "    return PIL_to_cv2(tensor_to_PIL(img))\n",
        "\n",
        "\n",
        "def PIL_to_cv2(img):\n",
        "    \"\"\"\n",
        "    Tool function to convert PIL image to cv2 image\n",
        "    :param img: PIL image\n",
        "    :return: cv2 image\n",
        "    \"\"\"\n",
        "    img = np.array(img)\n",
        "    img = img[:, :, ::-1].copy()\n",
        "    return img\n",
        "\n",
        "\n",
        "def cv2_to_PIL(img):\n",
        "    \"\"\"\n",
        "    Tool function to convert cv2 image to PIL image\n",
        "    :param img: cv2 image\n",
        "    :return: PIL image\n",
        "    \"\"\"\n",
        "    img = img[:, :, ::-1].copy()\n",
        "    img = Image.fromarray(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "# from d2l\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"A utility function to set matplotlib axes\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend: axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "\n",
        "# from d2l\n",
        "class Animator(object):\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
        "                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
        "        \"\"\"Incrementally plot multiple lines.\"\"\"\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1: self.axes = [self.axes,]\n",
        "        # use a lambda to capture arguments\n",
        "        self.config_axes = lambda : set_axes(\n",
        "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
        "        if not hasattr(y, \"__len__\"): y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"): x = [x] * n\n",
        "        if not self.X: self.X = [[] for _ in range(n)]\n",
        "        if not self.Y: self.Y = [[] for _ in range(n)]\n",
        "        if not self.fmts: self.fmts = ['-'] * n\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyC6H3p-smnl"
      },
      "source": [
        "### <b>Utility 라이브러리 정의</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LYPwfwOsqS1"
      },
      "outputs": [],
      "source": [
        "# from d2l\n",
        "class Accumulator(object):\n",
        "    \"\"\"\n",
        "    Sum a list of numbers over time\n",
        "    from: https://github.com/dsgiitr/d2l-pytorch/blob/master/d2l/base.py\n",
        "    \"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + b for a, b in zip(self.data, args)]\n",
        "    def reset(self):\n",
        "        self.data = [0] * len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i]\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer\"\"\"\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list\"\"\"\n",
        "        self.times.append(time.time() - self.start_time)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time\"\"\"\n",
        "        return sum(self.times)/len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumuated times\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8spcHHvwXsy"
      },
      "source": [
        "#### <b>Yolo 모델 코드</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1RCaZuZwahc"
      },
      "outputs": [],
      "source": [
        "lambda_coord = 5.\n",
        "lambda_noobj = .5\n",
        "\n",
        "\n",
        "class YoloBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YoloBackbone, self).__init__()\n",
        "        conv1 = nn.Sequential(\n",
        "            # [#, 448, 448, 3] => [#, 224, 224, 64]\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        # [#, 224, 224, 64] => [#, 112, 112, 64]\n",
        "        pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        conv2 = nn.Sequential(\n",
        "            # [#, 112, 112, 64] => [#, 112, 112, 192]\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        # [#, 112, 112, 192] => [#, 56, 56, 192]\n",
        "        pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        conv3 = nn.Sequential(\n",
        "            # [#, 56, 56, 192] => [#, 56, 56, 128]\n",
        "            nn.Conv2d(192, 128, kernel_size=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 56, 56, 128] => [#, 56, 56, 256]\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 56, 56, 256] => [#, 56, 56, 256]\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 56, 56, 256] => [#, 56, 56, 512]\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        # [#, 56, 56, 512] => [#, 28, 28, 512]\n",
        "        pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        conv4_part = nn.Sequential(\n",
        "            # [#, 28, 28, 512] => [#, 28, 28, 256]\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 28, 28, 256] => [#, 28, 28, 512]\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        conv4_modules = []\n",
        "        for _ in range(4):\n",
        "            conv4_modules.append(conv4_part)\n",
        "        conv4 = nn.Sequential(\n",
        "            *conv4_modules,\n",
        "            # [#, 28, 28, 512] => [#, 28, 28, 512]\n",
        "            nn.Conv2d(512, 512, kernel_size=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 28, 28, 512] => [#, 28, 28, 1024]\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        # [#, 28, 28, 1024] => [#, 14, 14, 1024]\n",
        "        pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # [#, 14, 14, 1024] => [#, 14, 14, 1024]\n",
        "        conv5 = nn.Sequential(\n",
        "            # [#, 14, 14, 1024] => [#, 14, 14, 512]\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 14, 14, 512] => [#, 14, 14, 1024]\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 14, 14, 1024] => [#, 14, 14, 512]\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 14, 14, 512] => [#, 14, 14, 1024]\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "        self.net = nn.Sequential(conv1, pool1, conv2, pool2, conv3, pool3, conv4, pool4, conv5)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "\n",
        "class Yolo(nn.Module):\n",
        "    def __init__(self, backbone: YoloBackbone, backbone_out_channels=1024):\n",
        "        super(Yolo, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = nn.Sequential(\n",
        "            # [#, 14, 14, ?] => [#, 14, 14, 1024]\n",
        "            nn.Conv2d(backbone_out_channels, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 14, 14, 1024] => [#, 7, 7, 1024]\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 7, 7, 1024] => [#, 7, 7, 1024]\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 7, 7, 1024] => [#, 7, 7, 1024]\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 7, 7, 1024] => [#, 7*7*1024]\n",
        "            nn.Flatten(),\n",
        "            # [#, 7*7*1024] => [#, 4096]\n",
        "            nn.Linear(7*7*1024, 4096),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            # [#, 4096] => [#, 7*7*30]\n",
        "            nn.Linear(4096, 7*7*30),\n",
        "            nn.Sigmoid(), #  normalize to [0, 1]\n",
        "            # [#, 7*7*30] => [#, 7, 7, 30]\n",
        "            nn.Unflatten(1, (7, 7, 30))\n",
        "        )\n",
        "        self.net = nn.Sequential(self.backbone, self.head)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "\n",
        "class YoloPretrain(nn.Module):\n",
        "    def __init__(self, backbone: YoloBackbone):\n",
        "        super(YoloPretrain, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = nn.Sequential(\n",
        "            # We use 224*224*3 to pretrain on ImageNet\n",
        "            # so the output is [#, 7, 7, 1024]\n",
        "            backbone,\n",
        "            # [#, 7, 7, 1024] => [#, 1, 1, 1024]\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            # [#, 1, 1, 1024] => [#, 1024]\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, 1000)\n",
        "        )\n",
        "        self.net = nn.Sequential(self.backbone, self.head)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "\n",
        "def yolo_loss(yhat, y):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        yhat: [#, 7, 7, 30]\n",
        "        y: [#, 7, 7, 30]\n",
        "    Returns:\n",
        "        loss: [#]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # arrange cell xidx, yidx\n",
        "        # [7, 7]\n",
        "        cell_xidx = (torch.arange(49) % 7).reshape(7, 7)\n",
        "        cell_yidx = (torch.div(torch.arange(49), 7, rounding_mode='floor')).reshape(7, 7)\n",
        "        # transform to [7, 7, 2]\n",
        "        cell_xidx.unsqueeze_(-1)\n",
        "        cell_yidx.unsqueeze_(-1)\n",
        "        cell_xidx.expand(7, 7, 2)\n",
        "        cell_yidx.expand(7, 7, 2)\n",
        "        # move to device\n",
        "        cell_xidx = cell_xidx.to(yhat.device)\n",
        "        cell_yidx = cell_yidx.to(yhat.device)\n",
        "\n",
        "    def calc_coord(val):\n",
        "        with torch.no_grad():\n",
        "            # transform cell relative coordinates to image relative coordinates\n",
        "            x = (val[..., 0] + cell_xidx) / 7.0\n",
        "            y = (val[..., 1] + cell_yidx) / 7.0\n",
        "\n",
        "            return (x - val[..., 2] / 2.0,\n",
        "                x + val[..., 2] / 2.0,\n",
        "                y - val[..., 3] / 2.0,\n",
        "                y + val[..., 3] / 2.0)\n",
        "\n",
        "    y_area = y[..., :10].reshape(-1, 7, 7, 2, 5)\n",
        "    yhat_area = yhat[..., :10].reshape(-1, 7, 7, 2, 5)\n",
        "\n",
        "    y_class = y[..., 10:].reshape(-1, 7, 7, 20)\n",
        "    yhat_class = yhat[..., 10:].reshape(-1, 7, 7, 20)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # calculate IoU\n",
        "        x_min, x_max, y_min, y_max = calc_coord(y_area)\n",
        "        x_min_hat, x_max_hat, y_min_hat, y_max_hat = calc_coord(yhat_area)\n",
        "\n",
        "        wi = torch.min(x_max, x_max_hat) - torch.max(x_min, x_min_hat)\n",
        "        wi = torch.max(wi, torch.zeros_like(wi))\n",
        "        hi = torch.min(y_max, y_max_hat) - torch.max(y_min, y_min_hat)\n",
        "        hi = torch.max(hi, torch.zeros_like(hi))\n",
        "\n",
        "        intersection = wi * hi\n",
        "        union = (x_max - x_min) * (y_max - y_min) + (x_max_hat - x_min_hat) * (y_max_hat - y_min_hat) - intersection\n",
        "        iou = intersection / (union + 1e-6) # add epsilon to avoid nan\n",
        "\n",
        "        _, res = iou.max(dim=3, keepdim=True)\n",
        "\n",
        "    # [#, 7, 7, 5]\n",
        "    # responsible bounding box (having higher IoU)\n",
        "    yhat_res = torch.take_along_dim(yhat_area, res.unsqueeze(3), 3).squeeze_(3)\n",
        "    y_res = y_area[..., 0, :5]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # calculate indicator matrix\n",
        "        have_obj = y_res[..., 4] > 0\n",
        "        no_obj = ~have_obj\n",
        "\n",
        "    return ((lambda_coord * ( # coordinate loss\n",
        "          (y_res[..., 0] - yhat_res[..., 0]) ** 2 # X\n",
        "        + (y_res[..., 1] - yhat_res[..., 1]) ** 2 # Y\n",
        "        + (torch.sqrt(y_res[..., 2]) - torch.sqrt(yhat_res[..., 2])) ** 2  # W\n",
        "        + (torch.sqrt(y_res[..., 3]) - torch.sqrt(yhat_res[..., 3])) ** 2) # H\n",
        "        # confidence\n",
        "        + (y_res[..., 4] - yhat_res[..., 4]) ** 2\n",
        "        # class\n",
        "        + ((y_class - yhat_class) ** 2).sum(dim=3)) * have_obj\n",
        "        # noobj\n",
        "        + ((y_area[..., 0, 4] - yhat_area[..., 0, 4]) ** 2 + \\\n",
        "        (y_area[..., 1, 4] - yhat_area[..., 1, 4]) ** 2) * no_obj * lambda_noobj).sum(dim=(1, 2))\n",
        "\n",
        "\n",
        "def pretrain(net, train_iter, test_iter, num_epochs, lr, momentum, weight_decay, device):\n",
        "    # init params\n",
        "    net.apply(weight_init)\n",
        "    # copy to device\n",
        "    net.to(device)\n",
        "    # define optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    # train\n",
        "    # TODO: IMPLEMENT HERE\n",
        "\n",
        "\n",
        "def train(net, train_iter, test_iter, num_epochs, lr, momentum, weight_decay, num_gpu=1, accum_batch_num=1, save_path='./model', load=None, load_epoch=-1, pretrained=False):\n",
        "    '''\n",
        "    Train net work. Some notes for load & load_epoch:\n",
        "    :param load: the file of model weights to load\n",
        "    :param load_epoch: num of epoch already completed (minus 1). should be the same with the number in auto-saved file name.\n",
        "    '''\n",
        "\n",
        "    def print_and_log(msg, log_file):\n",
        "        print(msg)\n",
        "        with open(log_file, 'a', encoding='utf8') as f:\n",
        "            f.write(msg + '\\n')\n",
        "\n",
        "    def update_lr(opt, lr):\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def get_all_gpu(num_gpu):\n",
        "        return [torch.device('cuda:' + str(i)) for i in range(num_gpu)]\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    log_file = os.path.join(save_path, f'log-{time.time_ns()}.txt')\n",
        "\n",
        "    if load:\n",
        "        net.load_state_dict(torch.load(load))\n",
        "    elif pretrained:\n",
        "        net.head.apply(weight_init)\n",
        "    else:\n",
        "        # init params\n",
        "        net.apply(weight_init)\n",
        "\n",
        "    # copy to device\n",
        "    if not torch.cuda.is_available():\n",
        "        net = net.to(torch.device('cpu'))\n",
        "        devices = [torch.device('cpu')]\n",
        "    else:\n",
        "        if num_gpu > 1:\n",
        "            net = nn.DataParallel(net, get_all_gpu(num_gpu))\n",
        "            devices = get_all_gpu(num_gpu)\n",
        "        else:\n",
        "            net = net.to(torch.device('cuda'))\n",
        "            devices = [torch.device('cuda')]\n",
        "    # define optimizer\n",
        "    if isinstance(lr, float):\n",
        "        tlr = lr\n",
        "    else: tlr = 0.001\n",
        "\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=tlr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # visualization\n",
        "    animator = Animator(xlabel='epoch', xlim=[0, num_epochs], legend=['train loss', 'test loss'])\n",
        "\n",
        "    num_batches = len(train_iter)\n",
        "    # train\n",
        "    for epoch in range(num_epochs - load_epoch - 1):\n",
        "        # adjust true epoch number according to pre_load\n",
        "        epoch = epoch + load_epoch + 1\n",
        "\n",
        "        # define metrics: train loss, sample count\n",
        "        metrics = Accumulator(2)\n",
        "        # define timer\n",
        "        timer = Timer()\n",
        "\n",
        "        # train\n",
        "        net.train()\n",
        "\n",
        "        # set batch accumulator\n",
        "        accum_cnt = 0\n",
        "        accum = 0\n",
        "\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            timer.start()\n",
        "\n",
        "            X, y = batch\n",
        "            X, y = X.to(devices[0]), y.to(devices[0])\n",
        "            yhat = net(X)\n",
        "\n",
        "            loss_val = yolo_loss(yhat, y)\n",
        "\n",
        "            # backward to accumulate gradients\n",
        "            loss_val.sum().backward()\n",
        "            # update batch accumulator\n",
        "            accum += 1\n",
        "            accum_cnt += loss_val.shape[0]\n",
        "            # step when accumulator is full\n",
        "            if accum == accum_batch_num or i == num_batches - 1:\n",
        "                # update learning rate per epoch and adjust by accumulated batch_size\n",
        "                if callable(lr):\n",
        "                    update_lr(optimizer, lr(epoch) / accum_cnt)\n",
        "                else:\n",
        "                    update_lr(optimizer, lr / accum_cnt)\n",
        "                # step\n",
        "                optimizer.step()\n",
        "                # clear\n",
        "                optimizer.zero_grad()\n",
        "                accum_cnt = 0\n",
        "                accum = 0\n",
        "\n",
        "            # update metrics\n",
        "            with torch.no_grad():\n",
        "                metrics.add(loss_val.sum().cpu(), X.shape[0])\n",
        "            train_l = metrics[0] / metrics[1]\n",
        "\n",
        "            timer.stop()\n",
        "\n",
        "            # log & visualization\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                print_and_log(\"epoch: %d, batch: %d / %d, loss: %.4f, time: %.4f\" % (epoch, i + 1, num_batches, train_l.item(), timer.sum()), log_file)\n",
        "                animator.add(epoch + (i + 1) / num_batches, (train_l, None))\n",
        "\n",
        "        # redefine metrics: test loss, test sample count\n",
        "        metrics = Accumulator(2)\n",
        "        # redefine timer\n",
        "        timer = Timer()\n",
        "        # test\n",
        "        net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            timer.start()\n",
        "\n",
        "            for batch in test_iter:\n",
        "                X, y = batch\n",
        "                X, y = X.to(devices[0]), y.to(devices[0])\n",
        "                yhat = net(X)\n",
        "\n",
        "                loss_val = yolo_loss(yhat, y)\n",
        "                metrics.add(loss_val.sum().cpu(), X.shape[0])\n",
        "\n",
        "            timer.stop()\n",
        "\n",
        "            test_l = metrics[0] / metrics[1]\n",
        "            print_and_log(\"epoch: %d, test loss: %.4f, time: %.4f\" % (epoch + 1, test_l.item(), timer.sum()), log_file)\n",
        "            animator.add(epoch + 1, (None, test_l))\n",
        "\n",
        "        # save model\n",
        "        torch.save(net.state_dict(), os.path.join(save_path, f'./{time.time_ns()}-epoch-{epoch}.pth'))\n",
        "\n",
        "\n",
        "def nms(pred, threshold=0.5):\n",
        "    '''\n",
        "    Non-maximum suppression directly for output.\n",
        "    :param pred: pred results\n",
        "    :param threshold:\n",
        "    :return:\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        pred = pred.reshape((-1, 30))\n",
        "        # [[idx, x, y, w, h, iou, score_cls]]\n",
        "        nms_data = [[] for _ in range(20)]\n",
        "        for i in range(pred.shape[0]):\n",
        "            cell = pred[i]\n",
        "            score, idx = torch.max(cell[10:30], dim=0)\n",
        "            idx = idx.item()\n",
        "            x, y, w, h, iou = cell[0:5].cpu().numpy()\n",
        "\n",
        "            nms_data[idx].append([i, x, y, w, h, iou, score.item()])\n",
        "            x, y, w, h, iou = cell[5:10].cpu().numpy()\n",
        "            nms_data[idx].append([i, x, y, w, h, iou, score.item()])\n",
        "\n",
        "        ret = torch.zeros_like(pred)\n",
        "        flag = torch.zeros(pred.shape[0], dtype=torch.bool)\n",
        "        for c in range(20):\n",
        "            c_nms_data = np.array(nms_data[c])\n",
        "\n",
        "            keep_index = _nms(c_nms_data, threshold)\n",
        "            keeps = c_nms_data[keep_index]\n",
        "\n",
        "            for keep in keeps:\n",
        "                i, x, y, w, h, iou, score = keep\n",
        "                i = int(i)\n",
        "\n",
        "                last_score, _ = torch.max(ret[i][10:30], dim=0)\n",
        "                last_iou = ret[i][4]\n",
        "\n",
        "                if score * iou > last_score * last_iou:\n",
        "                    flag[i] = False\n",
        "                if flag[i]: continue\n",
        "\n",
        "                ret[i][0:5] = torch.tensor([x, y, w, h, iou])\n",
        "                ret[i][10:30] = 0\n",
        "                ret[i][10 + c] = score\n",
        "\n",
        "                flag[i] = True\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "def _nms(data, threshold):\n",
        "    '''\n",
        "    Non-maximum suppression.\n",
        "    :param data: numpy data array (i, x, y, w, h, score_area, score_cls)\n",
        "    :param threshold:\n",
        "    :return: keep index array\n",
        "    '''\n",
        "    if len(data) == 0:\n",
        "        return []\n",
        "\n",
        "    # cell relative coordinates\n",
        "    cell_idx = data[:, 0]\n",
        "    x = data[:, 1]\n",
        "    y = data[:, 2]\n",
        "    # calculate cell index\n",
        "    xidx = cell_idx % 7\n",
        "    yidx = cell_idx // 7\n",
        "    # transform to image relative coordinates\n",
        "    x = (x + xidx) / 7.0\n",
        "    y = (y + yidx) / 7.0\n",
        "    # obtain image relative width & height\n",
        "    w = data[:, 3]\n",
        "    h = data[:, 4]\n",
        "    # calculate coordinates\n",
        "    x1 = x - w / 2\n",
        "    y1 = y - h / 2\n",
        "    x2 = x + w / 2\n",
        "    y2 = y + h / 2\n",
        "\n",
        "    score_area = data[:, 5]\n",
        "\n",
        "    areas = w * h\n",
        "\n",
        "    order = score_area.argsort()[::-1]\n",
        "    keep = []\n",
        "\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "        inds = np.where(ovr <= threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return keep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akfxrFhLsfa3"
      },
      "source": [
        "### <b>Yolo v1 학습 코드</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFScDeqXsJLn"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats = ['svg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JDvdp5tsUkB",
        "outputId": "c61f6c36-8906-470d-aa39-fe143081506d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ../data/VOCDetection/VOCtrainval_06-Nov-2007.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460032000/460032000 [00:22<00:00, 20546124.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/VOCDetection/VOCtrainval_06-Nov-2007.tar to ../data/VOCDetection/\n",
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar to ../data/VOCDetection/VOCtest_06-Nov-2007.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 451020800/451020800 [00:22<00:00, 20260746.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/VOCDetection/VOCtest_06-Nov-2007.tar to ../data/VOCDetection/\n",
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ../data/VOCDetection/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1999639040/1999639040 [01:32<00:00, 21538510.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/VOCDetection/VOCtrainval_11-May-2012.tar to ../data/VOCDetection/\n",
            "Using downloaded and verified file: ../data/VOCDetection/VOCtrainval_11-May-2012.tar\n",
            "Extracting ../data/VOCDetection/VOCtrainval_11-May-2012.tar to ../data/VOCDetection/\n"
          ]
        }
      ],
      "source": [
        "train_iter, test_iter, test_iter_raw = load_data_voc(batch_size=16, download=True)\n",
        "# train_iter, test_iter, test_iter_raw = load_data_voc(batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np_vYaYtsV3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2c304c-eb60-4a99-9c87-a2efd1956018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 110MB/s]\n"
          ]
        }
      ],
      "source": [
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "backbone = nn.Sequential(*list(resnet18.children())[:-2]) # remove avg pool and fc\n",
        "net = Yolo(backbone, backbone_out_channels=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "N9V-Q1ArsZTw",
        "outputId": "49bd3550-63b3-4cf0-a11c-ecaf4f5bbae0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4e7cb66cd24c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m145\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m145\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_batch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d2a1ae96f586>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, test_iter, num_epochs, lr, momentum, weight_decay, num_gpu, accum_batch_num, save_path, load, load_epoch, pretrained)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mtrain_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"229.425pt\" height=\"183.35625pt\" viewBox=\"0 0 229.425 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-09-03T07:32:41.127123</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 229.425 183.35625 \nL 229.425 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 145.8 \nL 222.225 145.8 \nL 222.225 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 26.925 145.8 \nL 26.925 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mc934dce376\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"26.925\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(23.74375 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 60.597414 145.8 \nL 60.597414 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"60.597414\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <g transform=\"translate(54.234914 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 94.269828 145.8 \nL 94.269828 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"94.269828\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(87.907328 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 127.942241 145.8 \nL 127.942241 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"127.942241\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <g transform=\"translate(121.579741 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 161.614655 145.8 \nL 161.614655 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"161.614655\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <g transform=\"translate(152.070905 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 195.287069 145.8 \nL 195.287069 7.2 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mc934dce376\" x=\"195.287069\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(185.743319 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(109.346875 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path d=\"M 26.925 120.477129 \nL 222.225 120.477129 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path id=\"ma067edd674\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma067edd674\" x=\"26.925\" y=\"120.477129\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 6 -->\n      <g transform=\"translate(13.5625 124.276348) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path d=\"M 26.925 90.142459 \nL 222.225 90.142459 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#ma067edd674\" x=\"26.925\" y=\"90.142459\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 8 -->\n      <g transform=\"translate(13.5625 93.941678) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path d=\"M 26.925 59.807789 \nL 222.225 59.807789 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#ma067edd674\" x=\"26.925\" y=\"59.807789\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 63.607008) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path d=\"M 26.925 29.47312 \nL 222.225 29.47312 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#ma067edd674\" x=\"26.925\" y=\"29.47312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 12 -->\n      <g transform=\"translate(7.2 33.272339) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 27.194379 13.5 \nL 27.463759 45.319645 \nL 27.733138 60.576299 \nL 28.002517 71.149501 \nL 28.271897 78.021867 \nL 28.541276 108.316404 \nL 28.810655 110.623357 \nL 29.080034 113.290163 \nL 29.349414 115.430519 \nL 29.618793 116.794851 \nL 29.888172 126.457898 \nL 30.157552 125.289707 \nL 30.426931 125.758465 \nL 30.69631 126.58918 \nL 30.96569 127.436883 \nL 31.235069 133.393706 \nL 31.504448 133.554655 \nL 31.773828 133.802652 \nL 32.043207 134.123342 \nL 32.312586 134.489819 \nL 32.581966 139.413877 \nL 32.851345 139.5 \nL 33.120724 139.141529 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 28.271897 119.076418 \nL 29.618793 127.835219 \nL 30.96569 134.235544 \nL 32.312586 137.100921 \n\" clip-path=\"url(#p63c11077b8)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 145.8 \nL 26.925 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 222.225 145.8 \nL 222.225 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 145.8 \nL 222.225 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 222.225 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 137.45625 44.55625 \nL 215.225 44.55625 \nQ 217.225 44.55625 217.225 42.55625 \nL 217.225 14.2 \nQ 217.225 12.2 215.225 12.2 \nL 137.45625 12.2 \nQ 135.45625 12.2 135.45625 14.2 \nL 135.45625 42.55625 \nQ 135.45625 44.55625 137.45625 44.55625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 139.45625 20.298438 \nL 149.45625 20.298438 \nL 159.45625 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train loss -->\n     <g transform=\"translate(167.45625 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 139.45625 34.976562 \nL 149.45625 34.976562 \nL 159.45625 34.976562 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- test loss -->\n     <g transform=\"translate(167.45625 38.476562) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"100.732422\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"152.832031\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"192.041016\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"223.828125\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"251.611328\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"312.792969\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"364.892578\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p63c11077b8\">\n   <rect x=\"26.925\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def lr(epoch):\n",
        "    if epoch < 10: return 0.001 * (epoch + 1)\n",
        "    if epoch < 85: return 0.01\n",
        "    if epoch < 115: return 0.001\n",
        "    if epoch < 145: return 0.0001\n",
        "\n",
        "train(net, train_iter, test_iter, 145, lr=lr, momentum=0.9, weight_decay=5e-4, accum_batch_num=4, save_path='./model', pretrained=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}